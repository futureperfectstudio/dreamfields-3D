{"cells":[{"cell_type":"markdown","metadata":{"id":"bhL1XJMXBZ5-"},"source":["# dreamfield-3D\n","\n","A toolkit to generate 3D mesh model / video / nerf instance / multiveiw images of colourful 3D objects by text and image prompts input. Edited by [Shengyu Meng (Simon)](https://twitter.com/meng_shengyu)   \n","Wellcome to share the outputs in social media with tag **#dreamfields3D**  ðŸ˜€"]},{"cell_type":"markdown","source":["## Update logs\n","\n","**Beta v0.65** ï¼šadd options for apply image prompts only in assigned direction to avoid overfitting; could skip saving depth maps in training and testing; output sequence images in validation process. (2022-10-03)  \n","**Beta v0.60** : apply random fovy (view angle) in training; update image augmentation before feed into CLIP; improve training stability and performance. (2022-09-25)"],"metadata":{"id":"2CjZVyyusnH9"}},{"cell_type":"markdown","metadata":{"id":"RXNlgTcGedPx"},"source":["## Credits & Changelog\n","Dreamfields-3D is modified from ashawkey's [dreamfields-torch](https://github.com/ashawkey/dreamfields-torch), and its forks of [IV Pravdin](https://github.com/ivpravdin) and\n","[Pollinations.AI](https://github.com/pollinations), released under [MIT License](https://github.com/ashawkey/dreamfields-torch/blob/main/LICENSE).\n","\n","The main improvements in this notebook & repository were done by [Shengyu Meng (Simon)](https://twitter.com/meng_shengyu), including allow visualizing training process in colab, export 360Â° video and 3D mesh model with vertex colour, and more arguments.This code is released under [MIT License](https://github.com/shengyu-meng/dreamfields-3D/blob/main/LICENSE)\n","\n","The [original dreamfields](https://ajayj.com/dreamfields) was issued under [Apache-2.0 license](https://github.com/google-research/google-research/blob/master/LICENSE), proposed by [Jain, Ajay ](https://ajayj.com/)and [Mildenhall, Ben](https://bmild.github.io/) and[ Barron, Jonathan T.](https://jonbarron.info/) and [Abbeel, Pieter](https://people.eecs.berkeley.edu/~pabbeel/) and[ Poole, Ben](https://cs.stanford.edu/~poole/) in their paper, [Zero-Shot Text-Guided Object Generation with Dream Fields](https://arxiv.org/abs/2112.01455) published on CVPR 2022. The main different of dreamfields-torch compared with original dreamfields is,  dreamfields-torch applied the [torch version of instant-ngp](https://github.com/ashawkey/torch-ngp) to replace the [original NeRF](https://github.com/bmild/nerf) as backend, and re-write most of the codes.\n","\n","The technical bases of original dreamfields are [NeRF: Neural Radiance Fields](https://github.com/bmild/nerf), released under [MIT License](https://github.com/bmild/nerf/blob/master/LICENSE), proposed by Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng in their paper [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2003.08934) published on ECCV 2020, and [CLIP: Connecting Text and Images model ](https://openai.com/blog/clip/)developed by [OpenAI](https://openai.com/), released under [MIT License](https://github.com/openai/CLIP/blob/main/LICENSE)."]},{"cell_type":"markdown","metadata":{"id":"lxZv-kV2V4NQ"},"source":["## check the machine"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"fRy9ezRGWJUb"},"outputs":[],"source":["#@title ### Check GPU\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"8TgTk0NUWSaU"},"outputs":[],"source":["#@title ### Mount Google drive (otpional)\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"Byj8OaoP0SHq"},"source":["## setup"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"BA86XROxbYEC"},"outputs":[],"source":["#@title ### pull the repostory & get working dir\n","import os \n","!git clone https://github.com/shengyu-meng/dreamfields-3D\n","%cd dreamfields-3D\n","DC_dir = os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"fHnwRDbDWhNw"},"outputs":[],"source":["#@title install dependencies\n","!pip install -r requirements.txt\n","!bash scripts/install_ext.sh\n","!bash scripts/install_PyMarchingCubes.sh"]},{"cell_type":"markdown","metadata":{"id":"wdTK4PFV7JKB"},"source":["## Run training and test in colab"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"xI8bL_MmWwaG"},"outputs":[],"source":["#@title ###Define necessary functions \n","\n","#find the path of marching_cubes\n","import site\n","import glob\n","import sys\n","\n","def find_pm_path(pm_path = '/usr/local/lib/python3.7/dist-packages/'):\n","  pm_path = pm_path + \"/PyMarchingCubes*/\"\n","  path = sorted(glob.glob(pm_path), reverse=True)\n","  if path:\n","      return path[0]\n","\n","pm_path = find_pm_path(site.getsitepackages()[0])\n","# marching_cubes_path = r\"/usr/local/lib/python3.7/dist-packages/PyMarchingCubes-0.0.2-py3.7-linux-x86_64.egg\" #it depend on your OS but just paste the path where is mcubes\n","if not pm_path in sys.path:\n","    sys.path.append(pm_path)    \n","import marching_cubes as mcubes\n","\n","\n","def get_latest_dir(path):\n","    dir_list = os.listdir(path)\n","    dir_list.sort(key=lambda x:os.path.getmtime((path+\"/\"+x)))\n","    assert len(dir_list) >= 1, \"there is not previous project in present output_dir, please check the path\" \n","    folder_name = os.path.join(output_dir, dir_list[-1])\n","    folder_name  = os.path.basename(folder_name)\n","    return folder_name\n","\n","def get_latest_file(path):\n","    dir_list = os.listdir(path)\n","    dir_list.sort(key=lambda x:os.path.getmtime((path+\"/\"+x)))\n","    return dir_list[-1]"]},{"cell_type":"markdown","metadata":{"id":"lBePhj67G6bs"},"source":["###**Training:**"]},{"cell_type":"markdown","metadata":{"id":"LZVFF4OnS-5z"},"source":["Parameters introductionï¼š\n","\n","*  Use Vit-B/16 in **clip_model** and 100 in **Epoch_num** for draft training; Vit-L/14 and 200 for fine training; the Vit-L/14 336 has not been tested yet.\n","*  check [here](https://openai.com/blog/clip/) about the performance for different **clip_model** , normally the model with better performance will cause more training time. \n","*  **clip_w_h** and **clip_aug_copy** will significantly affect the GPU RAM occupation and training speed, try to turn-in that with different combination, such as 232x12 \\ 225x16 \\ 168x20 \\ 128x24 in 16G RAM GPU.\n","*  **use_clip_direction_text** will determine if will add the prefix to the text prompt about camera direction, such as \"the font view of....\". This may reduce mismath but cause over fitting.\n","*  Strongly recommend to select **Prompt_image_direction** when used **Prompt_image** to avoid overfitting, to assign the view direction of image prompt (even with that the training with image prompt still quite not stable).\n","*  set **dt_gamma** >= 0 to adaptive ray marching, which will accelerate the training but possibly lead to worse result.\n","*  set **seed** to -1 to get random seed  \n","*  **clip_aug** is an experimental function about augment the image before input into CLIP; seems enable it to allow more details output and when turn off will train more stable.\n","*  **random_fovy_training** will apply random view angle in traning, to allow CLIP to observe the render in various scale.\n","*  larger **camerfa_fovy** amount will lead to smaller object seen in overall view.\n","*  set **resume_project** to \"latest\" to autoload the latest project in **output_dir**, otherwise indicate the specific project name.\n","*  The training outputs will be put in the subfolder with **project_name** plus timestamp when training start under **output_dir**; \n","*  **Attention:** Sometimes the colab UI will be stuck, but if the files under checkpoints folder is keep updating, the training is still running. You could wait to it finish then restart the kernel then run the **test** stage.\n","*  For the full usages, please check the [main_nerf.py](https://github.com/shengyu-meng/dreamfields-3D/blob/main/main_nerf.py) for detail."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1DUcMKvCW6if"},"outputs":[],"source":["# from typing_extensions import Text\n","#@markdown ####**Training Settings:**\n","Prompt_text = \"a organic biological pavilion could breath with building skin growing moss by Zaha Hadid, cgsociety, surreal, 8K HD\" #@param{type: 'string'}\n","Prompt_image = \"\"  #@param{type: 'string'}\n","Prompt_image_direction = 'None' #@param ['None','front', 'left side', 'back', 'right side', 'top', 'bottom']\n","Prompt_image_direction = \"\" if Prompt_image_direction == 'None' else \"--image_direction \" + \"'\" + Prompt_image_direction  + \"'\"\n","Epoch_num =  200 #@param {type: 'integer'}\n","learning_rate = 0.0005 #@param {type: 'number'}\n","dt_gamma = 0 #@param {type: 'number'}\n","render_W_H = 384  #@param {type: 'integer'}\n","clip_w_h = 224  #@param {type: 'integer'}\n","clip_aug_copy = 8 #@param {type: 'integer'}\n","clip_aug = True #@param {type:\"boolean\"}\n","clip_aug = \"--clip_aug \" if clip_aug else \"\"\n","use_clip_direction_text = True #@param {type:\"boolean\"}\n","use_clip_direction_text = \"--dir_text \" if use_clip_direction_text else \"\"\n","clip_model =  'ViT-L/14' #@param ['ViT-B/32','ViT-B/16','ViT-L/14','ViT-L/14@336px','RN50x16','RN50x64']\n","seed =  -1 #@param {type: 'integer'}\n","camera_fovy = 43 #@param {type: 'integer'}\n","random_fovy_training = True #@param {type:\"boolean\"}\n","random_fovy_training = \"--rnd_fovy\" if random_fovy_training else \"\"\n","##@markdown *  set seed to -1 to get random seed\n","resume_train = False #@param {type:\"boolean\"}\n","resume_project = \"latest\"  #@param{type: 'string'}\n","ckpt = \"latest\"  #@param{type: 'string'}\n","\n","#@markdown ---\n","\n","#@markdown ####**Output Settings:**\n","output_dir = \"/content/drive/MyDrive/Dreamfields3D_output/\" #@param{type: 'string'}\n","project_name = \"pavilion\"  #@param{type: 'string'}\n","display_interval = 12 #@param {type: \"integer\"}\n","interval_samples = 12 #@param {type: \"integer\"}\n","save_interval_img = False #@param {type:\"boolean\"}\n","save_interval_img = \"--save_interval_img\" if save_interval_img else \"\"\n","save_depth = False #@param {type:\"boolean\"}\n","save_depth = \"--save_depth\" if save_depth else \"\"\n","ckpt_save_interval = 50 #@param {type: \"integer\"}\n","mesh_resolution = 256 #@param {type: \"integer\"}\n","mesh_threshold = 10 #@param {type: \"integer\"}\n","# assign_subdir = True #@param {type:\"boolean\"}\n","\n","#paramater conversion\n","import time\n","import os\n","\n","#convert the prompt text\n","Prompt_text = \"'\" + Prompt_text + \"'\"\n","iteration = Epoch_num * 100\n","#get the workspace folder\n","\n","try:\n","    DC_dir\n","except NameError:\n","    DC_dir = '/content/dreamfields-3D/'\n","assert os.path.isfile (DC_dir + '/main_nerf.py'), \\\n","\"wrong working folder for dreamfileds3D, please enter the right folder and run again\"\n","\n","if not os.path.exists(output_dir):\n","    os.mkdir(output_dir)\n","\n","#check the parameters\n","assert clip_w_h % 8 ==0, f\"the value of clip_w_h should be divisible by 8, but current value {clip_w_h} is not, try to use 224 or 128\"\n","\n","#convert the augments\n","if not resume_train:\n","  workspace = project_name + \"_\" + time.strftime('%Y%m%d-%H%M%S')[4:-2]\n","\n","if resume_train:\n","  if resume_project == \"latest\":\n","    workspace = get_latest_dir(output_dir)\n","  else:\n","    workspace = resume_project\n","\n","if Prompt_image != \"\":\n","  Prompt_image = \"--image \" +  \"'\" + Prompt_image + \"'\"\n","\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mFMV_U0xhUj9"},"outputs":[],"source":["#@title Training\n","%cd {DC_dir}\n","\n","%run main_nerf.py --text {Prompt_text} {Prompt_image} {Prompt_image_direction} --cuda_ray --fp16 --iters {iteration} --seed {seed} \\\n","--output_dir {output_dir} --workspace {workspace} --colab --val_int {display_interval} --val_samples {interval_samples} --w {clip_w_h} --h {clip_w_h} \\\n","--H {render_W_H} --W {render_W_H} --fovy {camera_fovy} --clip_model {clip_model} --aug_copy {clip_aug_copy} --lr {learning_rate} --dt_gamma {dt_gamma} \\\n","--ckpt_save_interval {ckpt_save_interval} {save_interval_img} {use_clip_direction_text} {clip_aug} {random_fovy_training} --ckpt {ckpt} {save_depth} \n","%cd {DC_dir}"]},{"cell_type":"markdown","metadata":{"id":"8qC8IBGdHBu7"},"source":["###**Test:**\n","\n","Generate image / video / mesh with latest pretrained checkpoint file.\n","\n","*  set **test_project** to \"latest\" to autoload the latest project in output_dir, otherwise indicate the folder name of specific project (include the auto generate timestamp).\n","*  **test_samples** will control how many images will be generate, but only 20 will be displayed in colab, please check the output folder to view all images.\n","*  Check the **save_video** to generate the 360Â° video. The Video will be 20 FPS and the total frames of video will be as same as **test_samples**.\n","*  Check the **save_mesh** to generate obj and ply 3D model by marching cube algorithm. \n","*  The output models contain vertex colour, and could be directly view in meshlab and Rhino3D. For viewing the colour in Blender, please import the ply model (OBJ also work on latest blender 3.3), then create a new material for it, and plug a Color Attribute node into the new material in shader editor, then you should see the vertex colour."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_VDLFG_gUEKa"},"outputs":[],"source":["# from typing_extensions import Text\n","#@markdown ####**Test Settings:**\n","test_project = \"latest\"  #@param{type: 'string'}\n","ckpt = \"latest\"  #@param{type: 'string'}\n","test_samples = 240 #@param {type: \"integer\"}\n","render_W_H = 1024  #@param {type: 'integer'}\n","mesh_resolution = 256 #@param {type: \"integer\"}\n","mesh_threshold = 10 #@param {type: \"integer\"}\n","save_depth = False #@param {type:\"boolean\"}\n","save_depth = \"--save_depth\" if save_depth else \"\"\n","save_video = True #@param {type:\"boolean\"}\n","save_video = \"--save_video\" if save_video else \"\"\n","save_mesh = True #@param {type:\"boolean\"}\n","save_mesh = \"--save_mesh\" if save_mesh else \"\"\n","\n","#convert the parameters\n","if test_project == \"latest\" :\n","  workspace = get_latest_dir(output_dir)\n","else:\n","  workspace = test_project"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"JcMrBbvCbw5l"},"outputs":[],"source":["#@title Testing\n","#@ markdown Will generate images and mesh output use the latest checkpoint, even when the training has not finished.\n","%cd {DC_dir}\n","%run main_nerf.py --text {Prompt_text} --cuda_ray --fp16 --iters {iteration} --seed {seed} \\\n","--output_dir {output_dir} --workspace {workspace} --colab --val_int {display_interval} --w {clip_w_h} --h {clip_w_h} \\\n","--H {render_W_H} --W {render_W_H} --fovy {camera_fovy} --clip_model {clip_model} --aug_copy {clip_aug_copy} --dt_gamma 0 \\\n","--ckpt_save_interval {ckpt_save_interval} {save_interval_img} {use_clip_direction_text} {clip_aug} --ckpt {ckpt} {save_depth} \\\n","--test_samples {test_samples} --mesh_res {mesh_resolution} --mesh_trh {mesh_threshold}  {save_video} {save_mesh} \\\n","--test\n","%cd {DC_dir}"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"l0pD0Hhxpgrf"},"outputs":[],"source":["#@title display latest RGB video\n","from IPython.display import HTML\n","from base64 import b64encode\n","path2video =  f\"{output_dir}/{workspace}/videos/\" + get_latest_file (f\"{output_dir}/{workspace}/videos/\")\n","def show_video(video_path, video_width = 600):\n","   \n","  video_file = open(video_path, \"r+b\").read()\n"," \n","  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n","  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n"," \n","show_video(path2video)"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":["Byj8OaoP0SHq"],"provenance":[{"file_id":"1GrLcm79qW_QcR8fmxYWeKCajZxsT5ona","timestamp":1662385903411},{"file_id":"1KXbzDE7BVsfO9YBBhng8an2r1Mn9U-sn","timestamp":1662381757807},{"file_id":"17taXGRf557ZgekPFoWS1-_GZHdaine0r","timestamp":1662258701322},{"file_id":"1bluJTullAqpTrF4RUVrHfuTfKsI6yHgL","timestamp":1662124011749},{"file_id":"1T2BvOWuDrwJsAqM6tih0Kht7FLuGRRXl","timestamp":1661327047147}],"private_outputs":true},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}